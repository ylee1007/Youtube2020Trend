---
title: "tfidf_backup"
author: "youna(anna) kim"
date: "April 27, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(dplyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(tidyverse)
library(tidyr)
library(readxl)
library(sentimentr)
library(rlang)

library('cluster')    # clustering algorithms
library('factoextra') # clustering algorithms & visualization
library('quanteda')

```
# FOX

Get title from excel
```{r}

fox <- read_excel("fox_data_removedduplicates.xlsx")
docFox <-  fox[,1]
docFox$Title <-gsub("[[:punct:]]", "", docFox$Title)
docFox$Title <-gsub("fox", "", docFox$Title, ignore.case = TRUE) #dropping fox
docFox$Title <-gsub("[^0-9A-Za-z///' ]","", docFox$Title,ignore.case = TRUE)
docFox$Title <-gsub("Trumps", "Trump", docFox$Title, ignore.case = TRUE)


fox_title <- as.character(docFox$Title)
fox_title <- tibble(line= 1:1764, text=fox_title)
fox_title <- as.data.frame(fox_title)

```

word frequency
```{r}


fox_words <- fox_title %>%
  unnest_tokens(word, text)

fox_words <- fox_words%>%
  anti_join(stop_words)

fox_words %>%
  count(word, sort=TRUE)


fox_words %>%
  count(word, sort=TRUE)%>%
  filter(n>50)%>%
  mutate(word=reorder(word, n))%>%
  ggplot(aes(word, n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()+
  geom_text(aes(label=n),hjust=-0.3)

fox_bigrams <- fox_title%>%
  unnest_tokens(bigram, text, token="ngrams", n=2)

fox_bigrams %>%
  count(bigram, sort=TRUE)%>%
  separate(bigram, c("word1", "word2"), sep= " ")%>%
  filter(!word1 %in% stop_words$word)%>%
  filter(!word2 %in% stop_words$word)%>%
  unite(bigram, word1,word2, sep = " ")%>%
  filter(n>15)%>%
  mutate(word=reorder(bigram,n))%>%
  ggplot(aes(x=word, y=n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()+
  geom_text(aes(label=n), hjust=-0.3)

```

tf-idf
```{r}

library(quanteda)
library(caret)

TF = function(rownum){
  tf = rownum/sum(rownum)
  return(tf)
}
IDF = function(docnum){
  idf = log10((length(docnum))/(length(which(docnum > 0))))
  return(idf)
}

TF_IDF = function(tf,idf){
  tfidf = tf*idf
  return(tfidf)
}

```

Tokenize words and select top 1000 frequent words
```{r}
# tokenize
fox_title_token <- tokens(fox_title$text, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_hyphens = TRUE)

# clean tokens
fox_title_train <- fox_title_token %>% tokens_tolower() %>% tokens_select(stopwords(), selection = "remove") %>% tokens_wordstem(language="english")

fox_title_train <- tokens_ngrams(fox_title_train, n = 1:2) #phrase

fox_title_train_dfm <- dfm(fox_title_train, tolower = FALSE)

count1000<- topfeatures(fox_title_train_dfm, n = 1000, decreasing = TRUE, scheme = c("count", "docfreq"), group = NULL)

fox_title_top1000 <- names(count1000)

fox_title_top1000_dfm <- dfm_select(fox_title_train_dfm, pattern = fox_title_top1000)
fox_title_top1000_matrix <- as.matrix(fox_title_top1000_dfm)
fox_title_top1000_df <- as.data.frame(fox_title_top1000_matrix)
dim(fox_title_top1000_matrix)
```

apply tfidf
```{r}

fox.title_train_tf <- apply(fox_title_top1000_matrix, 1, TF)
fox.title_train_idf <- apply(fox_title_top1000_matrix, 2, IDF)
fox.title_train_tfidf <- apply(fox.title_train_tf, 2, TF_IDF, idf = fox.title_train_idf)
fox.train_token_df <- as.data.frame(t(fox.title_train_tfidf))
dim(fox.train_token_df)

```


```{r}

title.model_df <- fox.train_token_df

title.model_df$view <- fox$View
title.model_df$lognWord <- log(fox$`Title 1 Length`)

title.lm <- lm(view ~. , data = title.model_df)
anova(title.lm)
summary(title.lm)

```

k-means cluster
```{r}
library(textmineR)

# create a document term matrix 
dtm <- CreateDtm(doc_vec = fox_title$text, # character vector of documents
                 doc_names = fox_title$line, # document names
                 ngram_window = c(1, 2), # minimum and maximum n-gram length
                 stopword_vec = c(stopwords::stopwords("en"), # stopwords from tm
                                  stopwords::stopwords(source = "smart")), # this is the default value
                 lower = TRUE, # lowercase - this is the default value
                 remove_punctuation = TRUE, # punctuation - this is the default
                 remove_numbers = TRUE, # numbers - this is the default
                 verbose = FALSE, # Turn off status bar for this demo
                 cpus = 2) # default is all available cpus on the system

# construct the matrix of term counts to get the IDF vector
tf_mat <- TermDocFreq(dtm)


# TF-IDF and cosine similarity
tfidf <- t(dtm[ , tf_mat$term ]) * tf_mat$idf

tfidf <- t(tfidf)




csim <- tfidf / sqrt(rowSums(tfidf * tfidf))

csim <- csim %*% t(csim)


cdist <- as.dist(1 - csim)




hc <- hclust(cdist, "ward.D")

clustering <- cutree(hc, 10)

plot(hc, main = "Hierarchical clustering of 100 NIH grant abstracts",
     ylab = "", xlab = "", yaxt = "n")

rect.hclust(hc, 10, border = "red")




p_words <- colSums(dtm) / sum(dtm)

cluster_words <- lapply(unique(clustering), function(x){
  rows <- dtm[ clustering == x , ]
  
  # for memory's sake, drop all words that don't appear in the cluster
  rows <- rows[ , colSums(rows) > 0 ]
  
  colSums(rows) / sum(rows) - p_words[ colnames(rows) ]
})


# create a summary table of the top 5 words defining each cluster
cluster_summary <- data.frame(cluster = unique(clustering),
                              size = as.numeric(table(clustering)),
                              top_words = sapply(cluster_words, function(d){
                                paste(
                                  names(d)[ order(d, decreasing = TRUE) ][ 1:5 ], 
                                  collapse = ", ")
                              }),
                              stringsAsFactors = FALSE)
cluster_summary


for(i in 1:10){
  
# plot a word cloud of one cluster as an example
wordcloud::wordcloud(words = names(cluster_words[[ i ]]), 
                     freq = cluster_words[[ i ]], 
                     max.words = 50, 
                     random.order = FALSE, 
                     colors = c("red", "yellow", "blue"),
                     main = "Top words in cluster 100")


}


```

# tf-idf + kmeans

```{r}

# environment 
library(tm)
library(proxy)
library(dplyr)


```

TF Term Frequency
https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html

```{r}

# 
# 1. [TFIDF] :
# @vector = pass in a vector of documents  
TFIDF <- function(vector) {
    # tf 
    news_corpus  <- Corpus( VectorSource(vector) )
    control_list <- list(removePunctuation = TRUE, stopwords = TRUE, tolower = TRUE)
    tf <- TermDocumentMatrix(news_corpus, control = control_list) %>% as.matrix()

    # idf
    idf <- log( ncol(tf) / ( 1 + rowSums(tf != 0) ) ) %>% diag()
    return( crossprod(tf, idf) )
}

# tf-idf matrix using news' title 
fox_tf_idf <- TFIDF(fox_title$text)

# 2. [Cosine] :
# distance between two vectors
Cosine <- function(x, y) {
    similarity <- sum(x * y) / ( sqrt( sum(y ^ 2) ) * sqrt( sum(x ^ 2) ) )

    # given the cosine value, use acos to convert back to degrees
    # acos returns the radian, multiply it by 180 and divide by pi to obtain degrees
    return( acos(similarity) * 180 / pi )
}

# 3. calculate pair-wise distance matrix 
pr_DB$set_entry( FUN = Cosine , names = c("Cos"))
d1 <- dist(fox_tf_idf, method = "Cosine")
pr_DB$delete_entry("Cosine")

# 4. heirachical clustering 
cluster1 <- hclust(d1, method = "ward.D")
plot(cluster1)
rect.hclust(cluster1, 5)


# split into 10 clusters
groups1 <- cutree(cluster1, 5)

# you can look at the distribution size of each cluster 
# table(groups1)

fox.cluster.1 <- fox_title$text[groups1 == 1 ]
#length(fox.cluster.1)
fox.cluster.2 <- fox_title$text[groups1 == 2 ]
fox.cluster.3 <- fox_title$text[groups1 == 3 ]
fox.cluster.4 <- fox_title$text[groups1 == 4 ]
fox.cluster.5 <- fox_title$text[groups1 == 5 ]
```

Tried to cluster the first group -> failed :)
```{r}


# tf-idf matrix using news' title 
#fox_tf_idf.c1 <- TFIDF(fox.cluster.1)


# 3. calculate pair-wise distance matrix 
#pr_DB$set_entry( FUN = Cosine , names = c("Cosine"))
#d1.c1 <- dist(fox_tf_idf.c1, method = "Cosine")
#pr_DB$delete_entry("Cosine")

# 4. heirachical clustering 
#cluster1.c1 <- hclust(d1, method = "ward.D")
#plot(cluster1.c1)
#rect.hclust(cluster1.c1, 5)


# split into 10 clusters
#groups1.c1 <- cutree(cluster1.c1, 5)


#length(fox_title$text[groups1.c1 == 2 ])
# 1566
```


Graphs
```{r}
library(wordcloud)

wordcloud(fox.cluster.1, max.words = 100, min.freq = 3, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
wordcloud(fox.cluster.2, max.words = 100, min.freq = 3, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
wordcloud(fox.cluster.3, max.words = 100, min.freq = 3, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
wordcloud(fox.cluster.4, max.words = 100, min.freq = 3, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
wordcloud(fox.cluster.5, max.words = 100, min.freq = 3, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))


```

